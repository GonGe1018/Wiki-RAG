[**한국어**](./README.md)
[**English**](./README-en.md)

# Wiki-RAG

크롤링한 위키 데이터를 OpenAI의 GPT를 사용하여 요약하고, FAISS 인덱스를 기반으로 RAG 파이프라인을 구축합니다

***현재 나무위키 크롤링만 지원합니다***

[학교에서 발표한 자료](./Wiki-RAG%20PT.pdf)

---

## 📋 기능

1. **위키 크롤링 및 요약**:
   - URL과 문서 제목을 입력하여 위키 데이터를 크롤링.
   - 크롤링된 데이터를 요약하고 저장.

2. **RAG 파이프라인 구축**:
   - 크롤링된 데이터를 기반으로 FAISS 인덱스를 생성.

3. **질문-답변 시스템**:
   - FAISS 인덱스를 로드하여 사용자가 입력한 질문에 대해 `gpt-4o-mini` 기반 답변과 답변의 출처를 제공.

---

## 📂 프로젝트 구조

```plaintext
project/
├── app.py                 # Streamlit 앱 메인 파일
├── crawler.py             # WikiCrawler 정의
├── summarizer.py          # OpenAISummarizer 정의
├── rag_pipeline.py        # RAG 파이프라인 정의
├── requirements.txt       # 필요한 Python 패키지 목록
└── README.md              # 프로젝트 설명 파일
```

---

## 🛠️ 설치 및 실행

### 1. requirements 설치
```bash
pip install -r requirements.txt
```

### 2. Streamlit 앱 실행
```bash
streamlit run app.py
```

---

## 🖥️ 사용 방법

### 1. OpenAI API 키 입력

- 앱 실행 후, **사이드바**에서 OpenAI API 키를 입력합니다.

### 2. 크롤링 및 요약 실행
Selenium으로 위키를 크롤링하고 OpenAI API를 사용하여 문서 내용을 요약하여 저장하는 단계입니다.
- **입력값**
  - **크롤링 시작 URL**: 크롤링을 시작할 위키 URL ***(현재 나무위키만 지원).***
  - **시작 문서 제목**: 크롤링 시작 문서의 제목.
  - **크롤링 깊이**: 탐색할 최대 깊이.

### 3. RAG 파이프라인 구축
저장된 데이터를 바탕으로 벡터 검색이 가능하도록 FAISS를 사용하여 파이프라인을 구축합니다.
- **이미 크롤링된 데이터를 사용하려면 이 단계부터 시작해도 됩니다.**
- 크롤링 및 요약된 데이터가 저장된 폴더의 이름(`dir_name`)을 입력합니다.
  - 폴더 이름은 "크롤링 및 요약 실행" 단계에서 자동으로 생성됩니다.
- **RAG 파이프라인 인덱스 저장 경로**를 입력합니다.

### 4. RAG 파이프라인 로드 및 질문
구축한 RAG 파이프라인을 로드하여 사용할 수 있습니다.
- 저장된 인덱스를 드롭다운 메뉴에서 선택하여 로드합니다.
- 질문의 답변에는 답변에 사용된 출처가 반환되어 확인할 수 있습니다.

---

## 💡 Tip

1. **크롤링 depth 관련**:
   - 크롤링 깊이가 클수록 시간이 오래 걸릴 수 있습니다. 빠른 테스트를 원한다면 깊이를 1로 설정하세요.


2. **OpenAI API 키**:
   - API 키는 코드에 하드코딩하지 않고, Streamlit UI에서 입력하는 방식을 사용합니다.
